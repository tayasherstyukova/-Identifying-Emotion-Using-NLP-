{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdbb0f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mrams\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mrams\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mrams\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import model_selection, svm, naive_bayes\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984a9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '/Users/mrams/Downloads'\n",
    "os.chdir(path)\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "train['dataset'] = 'train'\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test['dataset'] = 'test'\n",
    "val = pd.read_csv(\"val.csv\")\n",
    "val['dataset'] = 'val'\n",
    "\n",
    "df = pd.concat([train, test, val], ignore_index=True, axis=0)\n",
    "\n",
    "\n",
    "def clean_text_remove_stop(df):\n",
    "    sentences = []\n",
    "    for i in range(0,len(df)):\n",
    "        sent=df[\"sentence\"][i]\n",
    "        sent=re.sub(r'[,.;@#?!&$\\-\\']+', ' ', sent, flags=re.IGNORECASE)\n",
    "        sent=re.sub(' +', ' ', sent, flags=re.IGNORECASE)\n",
    "        sent=re.sub(r'\\\"', ' ', sent, flags=re.IGNORECASE)\n",
    "        sent=re.sub(r'[^a-zA-Z]', \" \", sent, flags=re.VERBOSE)\n",
    "        sent=sent.replace(',', '')\n",
    "        sent=' '.join(sent.split())\n",
    "        sent=re.sub(\"\\n|\\r\", \"\", sent)\n",
    "        sent = ' '.join([word for word in sent.split() if word not in stopwords.words(\"english\")])\n",
    "        sentences.append(sent)\n",
    "    df['clean'] = sentences\n",
    "    df['token'] = [str(word_tokenize(entry)) for entry in df['sentence']]\n",
    "    return df\n",
    "\n",
    "def clean_text_keep_stop(df):\n",
    "    sentences = []\n",
    "    for i in range(0,len(df)):\n",
    "        sent=df[\"sentence\"][i]\n",
    "        sent=re.sub(r'[,.;@#?!&$\\-\\']+', ' ', sent, flags=re.IGNORECASE)\n",
    "        sent=re.sub(' +', ' ', sent, flags=re.IGNORECASE)\n",
    "        sent=re.sub(r'\\\"', ' ', sent, flags=re.IGNORECASE)\n",
    "        sent=re.sub(r'[^a-zA-Z]', \" \", sent, flags=re.VERBOSE)\n",
    "        sent=sent.replace(',', '')\n",
    "        sent=' '.join(sent.split())\n",
    "        sent=re.sub(\"\\n|\\r\", \"\", sent)\n",
    "        sentences.append(sent)\n",
    "    df['clean'] = sentences\n",
    "    df['token'] = [str(word_tokenize(entry)) for entry in df['sentence']]\n",
    "    return df\n",
    "\n",
    "def CountVect(df):\n",
    "    sent_list=[]\n",
    "    for i in range(0,len(df)):\n",
    "        sent_list.append(df['clean'][i])\n",
    "        \n",
    "    MyCountV=CountVectorizer(\n",
    "        input=\"content\", \n",
    "        lowercase=True)\n",
    "    MyDTM = MyCountV.fit_transform(sent_list)  # create a sparse matrix\n",
    "    MyDTM = MyDTM.toarray()  # convert to a regular array\n",
    "    ColumnNames=MyCountV.get_feature_names_out()\n",
    "    MyDTM_DF=pd.DataFrame(MyDTM,columns=ColumnNames)\n",
    "    return(MyDTM_DF)\n",
    "\n",
    "def tfidf(df):\n",
    "    sent_list=[]\n",
    "    for i in range(0,len(df)):\n",
    "        sent_list.append(df['clean'][i])\n",
    "   \n",
    "    MyVect_TF=TfidfVectorizer(input='content')\n",
    "    Vect = MyVect_TF.fit_transform(sent_list)\n",
    "    \n",
    "    ColumnNamesTF=MyVect_TF.get_feature_names_out()\n",
    "    DF_TF=pd.DataFrame(Vect.toarray(),columns=ColumnNamesTF)\n",
    "     \n",
    "    return (DF_TF)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b839e7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You would now fit the model with the X being train_df and y being trainLabel.\\nTo compute accuracy, you would predict test_df and compare to testLabel'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Here is an example of how to use the code above.\n",
    "Say you want to build a model with the input using tf-idf vectorizer and keeping \n",
    "stopwords. After running the code above, this is what you would run.'''\n",
    "\n",
    "\n",
    "clean = clean_text_keep_stop(df)\n",
    "tf_matrix = tfidf(df)\n",
    "\n",
    "train_clean = clean[clean['dataset'] == 'train']\n",
    "train_index = clean[clean['dataset'] == 'train'].index.values.astype(int)\n",
    "test_clean = clean[clean['dataset'] == 'test']\n",
    "test_index = clean[clean['dataset'] == 'test'].index.values.astype(int)\n",
    "val_clean = clean[clean['dataset'] == 'val']\n",
    "val_index = clean[clean['dataset'] == 'val'].index.values.astype(int)\n",
    "\n",
    "\n",
    "trainLabel = train_clean['emotion'].astype('category')\n",
    "testLabel = test_clean['emotion'].astype('category')\n",
    "valLabel = val_clean['emotion'].astype('category')\n",
    "\n",
    "train_df = tf_matrix.iloc[train_index]\n",
    "test_df = tf_matrix.iloc[test_index]\n",
    "val_df = tf_matrix.iloc[val_index]\n",
    "\n",
    "''' You would now fit the model with the X being train_df and y being trainLabel.\n",
    "To compute accuracy, you would predict test_df and compare to testLabel'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c0cbd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5145635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean['token']\n",
    "y = clean['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3925164d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     ['i', 'didnt', 'feel', 'humiliated']\n",
       "1        ['i', 'can', 'go', 'from', 'feeling', 'so', 'h...\n",
       "2        ['im', 'grabbing', 'a', 'minute', 'to', 'post'...\n",
       "3        ['i', 'am', 'ever', 'feeling', 'nostalgic', 'a...\n",
       "4                        ['i', 'am', 'feeling', 'grouchy']\n",
       "                               ...                        \n",
       "19995    ['im', 'having', 'ssa', 'examination', 'tomorr...\n",
       "19996    ['i', 'constantly', 'worry', 'about', 'their',...\n",
       "19997    ['i', 'feel', 'its', 'important', 'to', 'share...\n",
       "19998    ['i', 'truly', 'feel', 'that', 'if', 'you', 'a...\n",
       "19999    ['i', 'feel', 'like', 'i', 'just', 'wan', 'na'...\n",
       "Name: token, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e9f452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        sadness\n",
       "1        sadness\n",
       "2          anger\n",
       "3           love\n",
       "4          anger\n",
       "          ...   \n",
       "19995    sadness\n",
       "19996        joy\n",
       "19997        joy\n",
       "19998        joy\n",
       "19999        joy\n",
       "Name: emotion, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "358f4d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (14000,)\n",
      "Testing Data Shape:  (6000,)\n",
      "Training Data Shape Labels:  (14000,)\n",
      "Testing Data Shape Labels:  (6000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
    "print('Training Data Shape: ', X_train.shape)\n",
    "print('Testing Data Shape: ', X_test.shape)\n",
    "print('Training Data Shape Labels: ', y_train.shape)\n",
    "print('Testing Data Shape Labels: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f24a6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "y_train = Encoder.fit_transform(y_train)\n",
    "y_test = Encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f2d4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf_vect = TfidfVectorizer()\n",
    "Tf_vect.fit(clean['token'])\n",
    "\n",
    "X_train_idf = Tf_vect.transform(X_train)\n",
    "X_test_idf = Tf_vect.transform(X_test)\n",
    "# print(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63eddacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17068\n",
      "189149\n",
      "(20000, 17070)\n"
     ]
    }
   ],
   "source": [
    "print(len(Tf_vect.vocabulary_))\n",
    "\n",
    "# Python3 program for the above approach\n",
    "from itertools import chain\n",
    " \n",
    " \n",
    "# Function to print all unique keys\n",
    "# present in a list of dictionaries\n",
    "def UniqueKeys(arr):\n",
    " \n",
    "    # Stores the list of unique keys\n",
    "    res = list(set(chain.from_iterable(sub.keys() for sub in arr)))\n",
    " \n",
    "    # Print the list\n",
    "    print(len(str(res)))\n",
    " \n",
    "# Driver Code\n",
    "arr = [Tf_vect.vocabulary_]\n",
    "UniqueKeys(arr)\n",
    "print(tf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fbc1c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16553)\t0.1981264202604649\n",
      "  (0, 15312)\t0.06565577956563604\n",
      "  (0, 13822)\t0.3249007884669962\n",
      "  (0, 12118)\t0.13315478368316536\n",
      "  (0, 11586)\t0.16146482167187381\n",
      "  (0, 10421)\t0.14839018627080183\n",
      "  (0, 10349)\t0.08120684094156898\n",
      "  (0, 7916)\t0.09585775293047588\n",
      "  (0, 7373)\t0.10236194729244866\n",
      "  (0, 6725)\t0.19983368199073806\n",
      "  (0, 6394)\t0.16306647710159253\n",
      "  (0, 5651)\t0.3597302976025241\n",
      "  (0, 5532)\t0.04820856068024611\n",
      "  (0, 3894)\t0.6694434270579819\n",
      "  (0, 2037)\t0.10232419084270623\n",
      "  (0, 1593)\t0.24989167255481493\n",
      "  (0, 1498)\t0.1522900632794741\n",
      "  (0, 553)\t0.06210454620559892\n",
      "  (0, 35)\t0.11143967201444599\n",
      "  (1, 15101)\t0.16258703711964595\n",
      "  (1, 13969)\t0.49534410523132905\n",
      "  (1, 13845)\t0.23874163282593833\n",
      "  (1, 10953)\t0.34218418903520653\n",
      "  (1, 5878)\t0.24188546999617963\n",
      "  (1, 5532)\t0.11405885723529079\n",
      "  :\t:\n",
      "  (13998, 165)\t0.5266461424658212\n",
      "  (13999, 16794)\t0.20267230070103254\n",
      "  (13999, 16503)\t0.41091184318735813\n",
      "  (13999, 16492)\t0.4330165416756648\n",
      "  (13999, 15312)\t0.1799899660477918\n",
      "  (13999, 15101)\t0.125592914801443\n",
      "  (13999, 14611)\t0.2140171502309456\n",
      "  (13999, 9342)\t0.21089498429392026\n",
      "  (13999, 8884)\t0.1643618796908097\n",
      "  (13999, 8548)\t0.18758516480241214\n",
      "  (13999, 7916)\t0.08759540444379679\n",
      "  (13999, 7458)\t0.08398879536532892\n",
      "  (13999, 6830)\t0.3058707613765789\n",
      "  (13999, 6788)\t0.19141374714650103\n",
      "  (13999, 6305)\t0.1776294861930059\n",
      "  (13999, 5532)\t0.04405327937847909\n",
      "  (13999, 4509)\t0.23619245837085492\n",
      "  (13999, 3213)\t0.19808162616655828\n",
      "  (13999, 2912)\t0.26012746159065203\n",
      "  (13999, 2125)\t0.11852444957328467\n",
      "  (13999, 2037)\t0.09350447519619948\n",
      "  (13999, 912)\t0.10967845790967003\n",
      "  (13999, 832)\t0.1113640461741981\n",
      "  (13999, 553)\t0.05675151645400557\n",
      "  (13999, 35)\t0.10183425797879275\n"
     ]
    }
   ],
   "source": [
    "print(X_train_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75044050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(gamma='auto')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(gamma='auto')\n",
    "model.fit(X_train_idf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39d051b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 17068)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_idf.shape)\n",
    "print(y_test.shape)\n",
    "predictions = model.predict(X_test_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfe8d4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fbb446ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  34.050000000000004\n",
      "SVM Weighted Precision Scores ->  0.11594025000000001\n",
      "SVM Macro Precision Scores ->  0.05675\n",
      "SVM Recall Scores ->  0.3405\n",
      "SVM F1 Scores ->  0.17298060425214473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.00      0.00      0.00       758\n",
      "       anger       0.00      0.00      0.00       755\n",
      "        love       0.34      1.00      0.51      2043\n",
      "    surprise       0.00      0.00      0.00       492\n",
      "        fear       0.00      0.00      0.00      1741\n",
      "         joy       0.00      0.00      0.00       211\n",
      "\n",
      "    accuracy                           0.34      6000\n",
      "   macro avg       0.06      0.17      0.08      6000\n",
      "weighted avg       0.12      0.34      0.17      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Accuracy Score -> \",accuracy_score(y_test, predictions)*100)\n",
    "print(\"SVM Weighted Precision Scores -> \", precision_score(y_test, predictions, average = 'weighted',zero_division = 0))\n",
    "print(\"SVM Macro Precision Scores -> \", precision_score(y_test, predictions, average = 'macro', zero_division = 0))\n",
    "print(\"SVM Recall Scores -> \", recall_score(y_test, predictions, average = 'weighted', zero_division = 0))\n",
    "print(\"SVM F1 Scores -> \", f1_score(y_test, predictions, average = 'weighted', zero_division = 0))\n",
    "target_names = clean['emotion'].unique()\n",
    "print(classification_report(y_test, predictions, target_names=target_names, zero_division = 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6dde203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  62.8\n",
      "Naive Bayes Weighted Precision Scores ->  0.7179921158496885\n",
      "Naive Bayes Macro Precision Scores ->  0.6970658729529419\n",
      "Naive Bayes Recall Scores ->  0.628\n",
      "Naive Bayes F1 Scores ->  0.5319638032577784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.94      0.16      0.27       758\n",
      "       anger       0.99      0.11      0.20       755\n",
      "        love       0.57      0.98      0.72      2043\n",
      "    surprise       1.00      0.01      0.02       492\n",
      "        fear       0.69      0.89      0.78      1741\n",
      "         joy       0.00      0.00      0.00       211\n",
      "\n",
      "    accuracy                           0.63      6000\n",
      "   macro avg       0.70      0.36      0.33      6000\n",
      "weighted avg       0.72      0.63      0.53      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing with the naive bayes\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(X_train_idf, y_train)# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(X_test_idf)# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(y_test, predictions_NB)*100)\n",
    "print(\"Naive Bayes Weighted Precision Scores -> \", precision_score(y_test, predictions_NB, average = 'weighted',zero_division = 0))\n",
    "print(\"Naive Bayes Macro Precision Scores -> \", precision_score(y_test, predictions_NB, average = 'macro', zero_division = 0))\n",
    "print(\"Naive Bayes Recall Scores -> \", recall_score(y_test, predictions_NB, average = 'weighted', zero_division = 0))\n",
    "print(\"Naive Bayes F1 Scores -> \", f1_score(y_test, predictions_NB, average = 'weighted', zero_division = 0))\n",
    "target_names = clean['emotion'].unique()\n",
    "print(classification_report(y_test, predictions_NB, target_names=target_names, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d303eddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = LinearSVC(verbose=0)\n",
    "model_3.fit(train_df, trainLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a01219",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model_3.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54b57fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  88.85\n",
      "SVM Weighted Precision Scores ->  0.8869830358661898\n",
      "SVM Macro Precision Scores ->  0.8530136053037572\n",
      "SVM Recall Scores ->  0.8885\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.88      0.88      0.88       275\n",
      "       anger       0.86      0.84      0.85       224\n",
      "        love       0.90      0.93      0.92       695\n",
      "    surprise       0.80      0.75      0.77       159\n",
      "        fear       0.93      0.93      0.93       581\n",
      "         joy       0.75      0.61      0.67        66\n",
      "\n",
      "    accuracy                           0.89      2000\n",
      "   macro avg       0.85      0.82      0.84      2000\n",
      "weighted avg       0.89      0.89      0.89      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Accuracy Score -> \",accuracy_score(testLabel, predictions2)*100)\n",
    "print(\"SVM Weighted Precision Scores -> \", precision_score(testLabel, predictions2, average = 'weighted',zero_division = 0))\n",
    "print(\"SVM Macro Precision Scores -> \", precision_score(testLabel, predictions2, average = 'macro', zero_division = 0))\n",
    "print(\"SVM Recall Scores -> \", recall_score(testLabel, predictions2, average = 'weighted', zero_division = 0))\n",
    "target_names = clean['emotion'].unique()\n",
    "print(classification_report(testLabel, predictions2, target_names=target_names, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e51a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
